{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook details:\n",
    "* handles all or slice from dataset (random choice of rows)\n",
    "* text preprocessing: remove all urls, remove empty chunk rows\n",
    "* text is devided in chunks to avoid truncation by BERT model\n",
    "* no parametertunig of any of the models\n",
    "* dimensionality reduction is done with UMAP\n",
    "* clustering with HDBSCAN\n",
    "* labeling is done by counting the most common words in each cluster after lemmatization with Spacy\n",
    "* results and parameters for each run are saved in new folder\n",
    "--------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from terminal:\n",
    "# create a virtual environment (venv): python -m venv venv\n",
    "# activate the virtual environment (on Windows): venv\\Scripts\\activate\n",
    "# activate the virtual environment (on Unix or MacOS): source venv/bin/activate\n",
    "\n",
    "# install libraries from requirements.txt from notebook\n",
    "%pip install -r https://raw.githubusercontent.com/DorotaBjoorn/Text-Classification-LIA-project/main/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertTokenizerFast, AutoTokenizer, AutoModel\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import plotly.express as px\n",
    "from datetime import datetime\n",
    "import json\n",
    "import webbrowser\n",
    "\n",
    "from filtering import remove_urls\n",
    "from preprocessing import read_raw_data_to_df, prepare_df, add_text_chunks_to_df\n",
    "from cluster_labeling import c_tf_idf, extract_top_n_words_per_topic, extract_cluster_sizes\n",
    "from visualisation import create_topic_cluster_scatter, custom_scatter_layout\n",
    "from save_results import create_experiment_folder, save_plot, save_dataframe, save_parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create df from csv/tsv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_name = 'UMAP_HBDSCAN.ipynb'\n",
    "__file__ = Path(notebook_name).resolve() #create __file__ manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'forum_posts.tsv'\n",
    "df = read_raw_data_to_df(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df\n",
    "col_containing_text = 'text'\n",
    "# n_rows = 1000\n",
    "\n",
    "df = prepare_df(df, col_containing_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save text in chunks short enough for model to handle and add to df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "id_col = 'article_id'\n",
    "df = add_text_chunks_to_df(df = df, tokenizer=tokenizer, id_column_name = id_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate embeddings, add to df and save in datawarehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "chunk_list = df[\"text_chunk\"].to_list()\n",
    "embeddings = model.encode(chunk_list, show_progress_bar=True, normalize_embeddings=True)\n",
    "\n",
    "df['chunk_embedding'] = list(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data with embeddings for possible later use\n",
    "datawarehouse_folder = Path(__file__).parents[1] / 'datawarehouse'\n",
    "datawarehouse_folder.mkdir(parents=True, exist_ok=True)\n",
    "df.to_csv(f'{datawarehouse_folder}/{file_name}_chunked_embeddings.tsv', sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create folder for current experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = Path(__file__).parents[1]\n",
    "file_name = file_name\n",
    "exp_folder_path = create_experiment_folder(base_path, file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce embedings dimensionality with UMAP and cluster with HBDSCAN, add cluster label to df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_params = {\n",
    "    'n_neighbors':20,\n",
    "    'n_components':8,\n",
    "    'min_dist':0.05, \n",
    "    'metric':'cosine'\n",
    "}\n",
    "\n",
    "umap_embeddings = UMAP(**umap_params).fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hbdscan_params = {\n",
    "    'min_cluster_size':20,\n",
    "    'metric':'euclidean',\n",
    "    #'min_samples':40,\n",
    "    'gen_min_span_tree':True,\n",
    "    'prediction_data':True,       \n",
    "    'cluster_selection_method':'eom'\n",
    "}\n",
    "\n",
    "cluster = HDBSCAN(**hbdscan_params).fit(umap_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cluster_label'] = cluster.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate most frequent words for each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a df where all text_chunks for each cluster are joined\n",
    "clustered_docs_df = df.groupby(['cluster_label'], as_index = False).agg({'text_chunk': ' '.join})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatize clustered text_chunks and save as new column in df\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "clustered_docs_df['lemmatized_text_chunk'] = clustered_docs_df['text_chunk'].apply(lambda text: ' '.join(token.lemma_ for token in nlp(text)))\n",
    "# NOTE: words like jews, jewish are not lemmatized to jew, which should be further addressed with SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate cluster sizes and most common words for each cluster\n",
    "tf_idf, count = c_tf_idf(clustered_docs_df['lemmatized_text_chunk'].values, m=len(chunk_list))\n",
    "top_n_words = extract_top_n_words_per_topic(tf_idf, count, clustered_docs_df, n=10)\n",
    "cluster_words_df = extract_cluster_sizes(df)\n",
    "cluster_words_df['top_words'] = cluster_words_df['cluster_label'].apply(lambda label: [word for word, _ in top_n_words[label]])\n",
    "print(cluster_words_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare for visualization in 2D and gather all in df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the same paramters for 2D UMAP as for initial dim reduction except for n_components = 2 (=> 2 dim)\n",
    "umap_params_2D = umap_params.copy()\n",
    "umap_params_2D['n_components'] = 2\n",
    "\n",
    "umap_embeddings_2D = UMAP(**umap_params_2D).fit_transform(embeddings)\n",
    "\n",
    "df['umap_x'] = umap_embeddings_2D[:, 0]\n",
    "df['umap_y'] = umap_embeddings_2D[:, 1]\n",
    "\n",
    "df = df.merge(cluster_words_df, on='cluster_label').drop('cluster_size', axis=1) # *1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize topic clusters and save plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = create_topic_cluster_scatter(df = df, category = 'cluster_label')\n",
    "\n",
    "fig = custom_scatter_layout(fig = fig, plot_title = 'Blogg posts grouped by cluster', x_title = 'umap_x', y_title = 'umap_y')\n",
    "fig.show()\n",
    "\n",
    "save_plot(fig, exp_folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open figure in browser\n",
    "cluster_plot_path = exp_folder_path / 'fig_clustered_text_data.html'\n",
    "webbrowser.open(str(cluster_plot_path), new=2)  # 'new=2' opens in a new tab or window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save df with all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dataframe(df, exp_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save paramteres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'raw_data_file': file_name,\n",
    "    'tokenizer_for_creating_chunks': str(tokenizer),\n",
    "    'embeddings_model': str(model),\n",
    "    'umap_params': umap_params,\n",
    "    'hdbscan_params': hbdscan_params\n",
    "}\n",
    "\n",
    "save_parameters(params, exp_folder_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

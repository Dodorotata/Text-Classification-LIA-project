{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook details:\n",
    "* handles all or slice from dataset (random choice of rows)\n",
    "* text preprocessing: remove all urls, remove empty chunk rows\n",
    "* text is devided in chunks to avoid truncation by BERT model\n",
    "* no parametertunig of any of the models\n",
    "* dimensionality reduction is done with UMAP\n",
    "* clustering with HBDSCAN\n",
    "* labeling is done by counting the most common words in each cluster after lemmatization\n",
    "* results and parameters for each run are saved in folder\n",
    "--------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertTokenizerFast, AutoTokenizer, AutoModel\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import plotly.express as px\n",
    "from datetime import datetime\n",
    "import json\n",
    "import webbrowser\n",
    "\n",
    "from tools.text.filtering import remove_urls\n",
    "from preprocessing import read_raw_data_to_df, prepare_df, add_text_chunks_to_df\n",
    "from cluster_labeling import c_tf_idf, extract_top_n_words_per_topic, extract_cluster_sizes\n",
    "from visualisation import create_topic_cluster_scatter, custom_scatter_layout\n",
    "from save_results import create_experiment_folder, save_plot, save_dataframe, save_parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create df from csv/tsv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "__file__ = '/home/dorota/projects/python/investigations/dorota_lia/text_classification/src/UMAP_HBDSCAN.ipynb' #TODO remove when working from .py as __file__ is defined in .py but not .ipunb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'vnnforum_small.tsv'\n",
    "df = read_raw_data_to_df(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df\n",
    "col_containing_text = 'text'\n",
    "n_rows = 1000\n",
    "\n",
    "df = prepare_df(df, col_containing_text, n_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save text in chunks short enough for model to handle and add to df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "id_col = 'article_id'\n",
    "df = add_text_chunks_to_df(df = df, tokenizer=tokenizer, id_column_name = id_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate embeddings, add to df and save in datawarehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "chunk_list = df[\"text_chunk\"].to_list()\n",
    "embeddings = model.encode(chunk_list, show_progress_bar=True, normalize_embeddings=True)\n",
    "\n",
    "df['chunk_embedding'] = list(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datawarehouse_folder = Path(__file__).parents[1] / 'datawarehouse'\n",
    "datawarehouse_folder.mkdir(parents=True, exist_ok=True)\n",
    "df.to_csv(f'{datawarehouse_folder}/{file_name}_chunked_embeddings.tsv', sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ALT Read all data into df and extract embeddings array instead of the steps above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # provide data to run and save results if script run from here\n",
    "# file_name = 'vnnforum_small.tsv'\n",
    "# tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# __file__ = '/home/dorota/projects/python/investigations/dorota_lia/text_classification/src/UMAP_HBDSCAN.ipynb' #TODO remove when working from .py as __file__ is defined in .py but not .ipunb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datawarehouse_folder = Path(__file__).parents[1] / 'datawarehouse'\n",
    "# file_name = 'vnnforum_small.tsv'\n",
    "# df = pd.read_csv(f'{datawarehouse_folder}/{file_name}_chunked_embeddings.tsv', sep=\"\\t\")\n",
    "\n",
    "# chunk_list = df[\"text_chunk\"].to_list()\n",
    "# embeddings = df['chunk_embedding'].apply(lambda x: np.fromstring(x[1:-1], sep=' ')).tolist()\n",
    "# embeddings = np.array(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create folder for current experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = Path(__file__).parents[1]\n",
    "file_name = file_name\n",
    "exp_folder_path = create_experiment_folder(base_path, file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce embedings dimensionality with UMAP and cluster with HBDSCAN, add cluster label to df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_params = {\n",
    "    'n_neighbors':20,\n",
    "    'n_components':8,\n",
    "    'min_dist':0.05, \n",
    "    'metric':'cosine'\n",
    "}\n",
    "\n",
    "umap_embeddings = UMAP(**umap_params).fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hbdscan_params = {\n",
    "    'min_cluster_size':20,\n",
    "    'metric':'euclidean',\n",
    "    #'min_samples':40,\n",
    "    'gen_min_span_tree':True,\n",
    "    'prediction_data':True,       \n",
    "    'cluster_selection_method':'eom'\n",
    "}\n",
    "\n",
    "cluster = HDBSCAN(**hbdscan_params).fit(umap_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cluster_label'] = cluster.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate most frequent words for each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a df where all text_chunks for each cluster are joined\n",
    "clustered_docs_df = df.groupby(['cluster_label'], as_index = False).agg({'text_chunk': ' '.join})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatize clustered text_chunks and save as new column in df\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "clustered_docs_df['lemmatized_text_chunk'] = clustered_docs_df['text_chunk'].apply(lambda text: ' '.join(token.lemma_ for token in nlp(text)))\n",
    "# NOTE: words like jews, jewish are not lemmatized to jew, which should be further addressed with SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate cluster sizes and most common words for each cluster\n",
    "tf_idf, count = c_tf_idf(clustered_docs_df['lemmatized_text_chunk'].values, m=len(chunk_list))\n",
    "top_n_words = extract_top_n_words_per_topic(tf_idf, count, clustered_docs_df, n=10)\n",
    "cluster_words_df = extract_cluster_sizes(df)\n",
    "cluster_words_df['top_words'] = cluster_words_df['cluster_label'].apply(lambda label: [word for word, _ in top_n_words[label]])\n",
    "print(cluster_words_df)\n",
    "\n",
    "# *1) merging here results in scattered clusters in visualization step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare for visualization in 2D and gather all in df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the same paramters for 2D UMAP as for initial dim reduction except for n_components = 2 (=> 2 dim)\n",
    "umap_params_2D = umap_params.copy()\n",
    "umap_params_2D['n_components'] = 2\n",
    "\n",
    "umap_embeddings_2D = UMAP(**umap_params_2D).fit_transform(embeddings)\n",
    "\n",
    "df['umap_x'] = umap_embeddings_2D[:, 0]\n",
    "df['umap_y'] = umap_embeddings_2D[:, 1]\n",
    "\n",
    "df = df.merge(cluster_words_df, on='cluster_label').drop('cluster_size', axis=1) # *1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize topic clusters and save plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = create_topic_cluster_scatter(df = df, category = 'cluster_label')\n",
    "\n",
    "fig = custom_scatter_layout(fig = fig, plot_title = 'VNN blogg grouped by cluster', x_title = 'umap_x', y_title = 'umap_y')\n",
    "fig.show()\n",
    "\n",
    "save_plot(fig, exp_folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open figure in browser\n",
    "cluster_plot_path = exp_folder_path / 'fig_clustered_text_data.html'\n",
    "webbrowser.open(str(cluster_plot_path), new=2)  # 'new=2' opens in a new tab or window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save df with all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dataframe(df, exp_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save paramteres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'raw_data_file': file_name,\n",
    "    'tokenizer_for_creating_chunks': str(tokenizer),\n",
    "    'embeddings_model': str(model),\n",
    "    'umap_params': umap_params,\n",
    "    'hdbscan_params': hbdscan_params\n",
    "}\n",
    "\n",
    "save_parameters(params, exp_folder_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
